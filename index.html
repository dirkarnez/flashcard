<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Large Language Models Flashcards</title>
    <style>
        body {
            font-family: 'Arial', sans-serif;
            background: linear-gradient(to bottom, #2c3e50, #3498db);
            display: flex;
            justify-content: center;
            align-items: center;
            height: 100vh;
            margin: 0;
            color: white;
        }
        .container {
            display: flex;
            flex-direction: column;
            align-items: center;
        }
        .flashcard {
            background: white;
            color: black;
            width: 300px;
            height: 200px;
            border-radius: 10px;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
            display: flex;
            justify-content: center;
            align-items: center;
            text-align: center;
            margin: 20px;
            cursor: pointer;
            transition: transform 0.6s;
            transform-style: preserve-3d;
            position: relative;
        }
        .flashcard.is-flipped {
            transform: rotateY(180deg);
        }
        .flashcard .front, .flashcard .back {
            position: absolute;
            width: 100%;
            height: 100%;
            backface-visibility: hidden;
            border-radius: 10px;
            display: flex;
            justify-content: center;
            align-items: center;
            padding: 20px;
            box-sizing: border-box;
        }
        .flashcard .back {
            transform: rotateY(180deg);
            background: #f7f7f7;
        }
        .button-container {
            display: flex;
            justify-content: space-between;
            width: 300px;
        }
        .button {
            background: #3498db;
            color: white;
            border: none;
            padding: 10px 20px;
            border-radius: 5px;
            cursor: pointer;
            transition: background 0.3s;
        }
        .button:hover {
            background: #2980b9;
        }
    </style>
</head>
<body>
    <div class="container">
        <div id="flashcard" class="flashcard">
            <div class="front">
                <h2 id="topic">Large Language Models</h2>
            </div>
            <div class="back">
                <p id="info">Large language models (LLMs) are artificial intelligence systems designed to understand and generate human language based on large datasets.</p>
            </div>
        </div>
        <div class="button-container">
            <button class="button" id="prev">Previous</button>
            <button class="button" id="next">Next</button>
        </div>
    </div>

    <script>
        const flashcards = [
            { topic: "Large Language Models", info: "Large language models (LLMs) are artificial intelligence systems designed to understand and generate human language based on large datasets." },
            { topic: "Transformer Architecture", info: "A neural network architecture that uses self-attention mechanisms to process input data and is the basis for many LLMs." },
            { topic: "GPT (Generative Pre-trained Transformer)", info: "A type of LLM developed by OpenAI that generates human-like text based on input prompts." },
            { topic: "BERT (Bidirectional Encoder Representations from Transformers)", info: "A transformer-based model designed for natural language understanding tasks." },
            { topic: "Pre-training", info: "The process of training a language model on a large corpus of text data before fine-tuning it for specific tasks." },
            { topic: "Fine-tuning", info: "The process of adjusting a pre-trained model on a smaller, task-specific dataset to improve performance on a particular application." },
            { topic: "Self-Attention Mechanism", info: "A technique used in transformers to weigh the importance of different words in a sentence when making predictions." },
            { topic: "Tokenization", info: "The process of converting text into smaller units (tokens) that can be processed by a language model." },
            { topic: "Zero-shot Learning", info: "The ability of a model to perform a task without having been explicitly trained on that task." },
            { topic: "Few-shot Learning", info: "The ability of a model to perform a task with only a few examples or demonstrations." },
            { topic: "Natural Language Processing (NLP)", info: "A field of AI focused on the interaction between computers and human language." },
            { topic: "Contextual Embeddings", info: "Representations of words that capture their meanings in different contexts, used in LLMs for better understanding." },
            { topic: "Language Model Evaluation", info: "The process of assessing the performance of a language model using metrics like perplexity, accuracy, and BLEU score." },
            { topic: "Ethical Considerations", info: "The study of the moral implications and responsibilities related to the development and deployment of LLMs, such as bias and fairness." },
            { topic: "Transfer Learning", info: "A machine learning technique where a model developed for one task is reused on a second, related task." },
            { topic: "Overfitting", info: "A modeling error that occurs when a model learns the training data too well, including its noise, leading to poor performance on new data." },
            { topic: "Regularization", info: "Techniques used to prevent overfitting by penalizing complex models, ensuring they generalize better to unseen data." },
            { topic: "Attention Heads", info: "Components of the transformer architecture that process different parts of the input data in parallel to identify relationships between words." },
            { topic: "Sequence-to-Sequence Models", info: "A type of model used for tasks like translation, where an input sequence is transformed into an output sequence." },
            { topic: "Masked Language Modeling", info: "A training task where some words in a sentence are masked and the model must predict them, used in models like BERT." }
        ];

        let currentIndex = 0;

        const flashcard = document.getElementById('flashcard');
        const topicElement = document.getElementById('topic');
        const infoElement = document.getElementById('info');

        function updateFlashcard(index) {
            topicElement.textContent = flashcards[index].topic;
            infoElement.textContent = flashcards[index].info;
        }

        flashcard.addEventListener('click', () => {
            flashcard.classList.toggle('is-flipped');
        });

        document.getElementById('next').addEventListener('click', () => {
            currentIndex = (currentIndex + 1) % flashcards.length;
            updateFlashcard(currentIndex);
        });

        document.getElementById('prev').addEventListener('click', () => {
            currentIndex = (currentIndex - 1 + flashcards.length) % flashcards.length;
            updateFlashcard(currentIndex);
        });

        updateFlashcard(currentIndex);
    </script>
</body>
</html>
